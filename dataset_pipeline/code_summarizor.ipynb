{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Extract the code segment between two \"--Call--\" lines in a log file\n",
    "def extract_call_segment(log_file_path, output_file_path):\n",
    "    with open(log_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.rstrip('\\n') for line in lines]\n",
    "\n",
    "    # Find the first and second occurrence of \"--Call--\"\n",
    "    call_indices = [i for i, line in enumerate(lines) if line.strip() == \"--Call--\"]\n",
    "    if len(call_indices) == 0:\n",
    "        print(\"Error: No '--Call--' found in the log file.\")\n",
    "        return\n",
    "    start_idx = call_indices[0]\n",
    "    if len(call_indices) > 1:\n",
    "        end_idx = call_indices[1]\n",
    "    else:\n",
    "        end_idx = len(lines)  # till end if second call not found\n",
    "\n",
    "    # Extract the segment between first and second call\n",
    "    segment = lines[start_idx:end_idx]\n",
    "    # Now we need to find lines that end with \"##line:(N)\" and then append the next (Pdb) dictionary\n",
    "    # Pattern to look for:\n",
    "    # Code line example: \"-> return [dictionary.get(key) for key in key_list]    ##line:(2)\"\n",
    "    # Followed by: \"(Pdb) {'key_list': [...], 'dictionary': {...}}\"\n",
    "    # We remove \"(Pdb)\" and append this dict to the previous line.\n",
    "    output_lines = []\n",
    "    i = 0\n",
    "    while i < len(segment):\n",
    "        line = segment[i]\n",
    "        if \"##line:(\" in line:\n",
    "            code_line = line\n",
    "            # Look ahead to find next line with (Pdb) { ... }\n",
    "            if i + 1 < len(segment) and segment[i+1].startswith(\"(Pdb) \"):\n",
    "                pdb_line = segment[i+1]\n",
    "                pdb_line_stripped = pdb_line.replace(\"(Pdb) \", \"\", 1)\n",
    "                # Append this to code_line\n",
    "                code_line += \" \" + pdb_line_stripped\n",
    "                i += 2  \n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "            output_lines.append(code_line)\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # Write the output lines to the file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        for line in output_lines:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "def extract_call_code_only(log_file_path, output_file_path):\n",
    "    with open(log_file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.rstrip('\\n') for line in lines]\n",
    "\n",
    "    # Find the first and second occurrence of \"--Call--\"\n",
    "    call_indices = [i for i, line in enumerate(lines) if line.strip() == \"--Call--\"]\n",
    "    if len(call_indices) == 0:\n",
    "        print(\"Error: No '--Call--' found in the log file.\")\n",
    "        return\n",
    "    start_idx = call_indices[0]\n",
    "    if len(call_indices) > 1:\n",
    "        end_idx = call_indices[1]\n",
    "    else:\n",
    "        end_idx = len(lines)  # till end if second call not found\n",
    "\n",
    "    # Extract the segment between first and second call\n",
    "    segment = lines[start_idx:end_idx]\n",
    "    output_lines = []\n",
    "    i = 0\n",
    "    while i < len(segment):\n",
    "        line = segment[i]\n",
    "        if \"##line:(\" in line:\n",
    "            code_line = line\n",
    "            output_lines.append(code_line)\n",
    "        i += 1\n",
    "\n",
    "    # Write the output lines to the file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        for line in output_lines:\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "extract_call_code_only(\"chunk_1_2/669532/logger_PDBscript_669532.txt\", \"test_extract_content.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utile import *\n",
    "\n",
    "def extract_all_call_segments(dataset_path, pdb_dir, output_dir):\n",
    "    jsons = read_jsonl_file(dataset_path)\n",
    "    for problem in jsons:\n",
    "        id = problem['id']\n",
    "        # Create a directory for the problem\n",
    "        os.makedirs(f\"{output_dir}/{id}\", exist_ok=True)\n",
    "        extract_call_segment(f\"{pdb_dir}/{id}/logger_PDBscript_{id}.txt\", f\"{output_dir}/{id}/code_values_{id}.txt\")\n",
    "        extract_call_code_only(f\"{pdb_dir}/{id}/logger_PDBscript_{id}.txt\", f\"{output_dir}/{id}/code_only_{id}.txt\")\n",
    "\n",
    "for i in range(1,11):    \n",
    "    extract_all_call_segments(f\"star_coder/chunk_4/chunk_4_{i}.jsonl\", f\"chunk_4_{i}\", \"code_sum_chunk4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> def repeat_string(s):    ##line:(1) {'s': 'abc', 'repeat': <class 'itertools.repeat'>}\n",
      "-> return ' '.join(repeat(s, 3))    ##line:(2) {'s': 'abc', 'repeat': <class 'itertools.repeat'>}\n",
      "-> return ' '.join(repeat(s, 3))    ##line:(2) {'s': 'abc', 'repeat': <class 'itertools.repeat'>, '__return__': 'abc abc abc'}\n"
     ]
    }
   ],
   "source": [
    "# Load the code lines from a txt as a string(skip the empty files and files with lines >15)\n",
    "def load_code_lines(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.rstrip('\\n') for line in lines]\n",
    "    if len(lines) > 15 or not lines:\n",
    "        return None\n",
    "    # Join the lines into a single string with newlines\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "print(load_code_lines(\"code_sum_chunk1/63/code_values_63.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of problems in the merged dictionary: 7255\n"
     ]
    }
   ],
   "source": [
    "# Merge the code_sum_chunks (get rid of the empty and long files)\n",
    "def merge_code_sum_chunk(chunk_dir, output_file):\n",
    "# traverse the chunk_dir\n",
    "    merged_dict = {}\n",
    "    for subdir in os.listdir(chunk_dir):\n",
    "        subdir_path = os.path.join(chunk_dir, subdir)\n",
    "        # the name of the subdir is the problem id\n",
    "        id = subdir\n",
    "        code_dict = {}\n",
    "        # if there is no code_only or code_values file, skip\n",
    "        if not os.path.exists(f\"{subdir_path}/code_only_{id}.txt\") or not os.path.exists(f\"{subdir_path}/code_values_{id}.txt\"):\n",
    "            continue\n",
    "        code_dict[\"code_only\"] = load_code_lines(f\"{subdir_path}/code_only_{id}.txt\")\n",
    "        code_dict[\"code_values\"] = load_code_lines(f\"{subdir_path}/code_values_{id}.txt\")\n",
    "        if code_dict[\"code_only\"] and code_dict[\"code_values\"]:\n",
    "            merged_dict[id] = code_dict\n",
    "    \n",
    "    # Print the number of problems in the merged dictionary\n",
    "    print(f\"Number of problems in the merged dictionary: {len(merged_dict)}\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(merged_dict, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "merge_code_sum_chunk(\"code_sum_chunk4\", \"code_sum_chunk4.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptcorrecter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
